#include "vision_based_tunnel_localizer.hh"
#include <opencv2/core/eigen.hpp>

VisionBasedTunnelLocalizer::VisionBasedTunnelLocalizer(int num_feats, int history_len, 
                       								string detector_type, double threshold){
	_feat_buffer_idx = 0;
    _num_feats = num_feats;
    _of_history_len = history_len;
    _detector_type = detector_type;
    _feat_threshold = threshold;
    _octree = NULL;
    _x_disp = 0;
    _fim  = Eigen::Matrix6d::Zero();
    _extractor = UniformFeatureExtractor(_detector_type, 3,_num_feats, Size2i(4, 3), _feat_threshold, true);
    _num_frames_pushed = 0;
}

bool VisionBasedTunnelLocalizer::push_image_data(const vector<cv::Mat> &frames, const Eigen::Matrix4d &pose){
    // This function requires the same number of calibration
    // data with the number of frames to be used in optical flow
    // and displacement estimation.
    assert(_cam_params.size() == frames.size());

    // ### Parallelize this part with boost or p_thread.
    for(int t = 0 ; t < (int)_trackers.size() ; t++){
        _trackers[t].track_features(frames[t], _extractor);
        if(_feats[!_feat_buffer_idx][t].size() == 0){
            _trackers[t].get_features(   _feats[!_feat_buffer_idx][t], 
                                      _feat_ids[!_feat_buffer_idx][t], 0);
            cv::undistortPoints(_feats[!_feat_buffer_idx][t], _feats[!_feat_buffer_idx][t], 
                                _cam_params[t].camera_matrix, _cam_params[t].dist_coeffs);
            _poses[!_feat_buffer_idx] = pose;
        }
    }

    _num_frames_pushed++;
    _poses[_feat_buffer_idx] = pose;

    if(_num_frames_pushed > _of_history_len)
        cout << "WARNING!!! # of frame pushed is larger than the history length" << endl;

    return true;
}

bool VisionBasedTunnelLocalizer::register_camera_params(const vector<CameraCalibParams> &params){
    reset();
	Eigen::Matrix3d temp_cam_mat;
    for(int i = 0 ; i < (int)params.size() ; i++){
        _cam_params.push_back(params[i]); // ### This is supposed to carry a deep copy.
        _trackers.push_back(UniformFeatureTracker(_num_feats, _of_history_len));
		cv2eigen(params[i].camera_matrix, temp_cam_mat);
        _inv_camera_matrices.push_back(temp_cam_mat.inverse());
	}

	_images.resize(params.size());

	_feats[0].resize(params.size());
	_feats[1].resize(params.size());
	_feat_ids[0].resize(params.size());
	_feat_ids[1].resize(params.size());
    return true;
}

bool VisionBasedTunnelLocalizer::reset(){
	_trackers.clear();
    _feat_buffer_idx = 0;
    _feats[0].clear();
    _feats[1].clear();
    _feat_ids[0].clear();
    _feat_ids[1].clear();
    _images.clear();
    _cam_params.clear();
    _inv_camera_matrices.clear();
    _num_frames_pushed = 0;
    // ### How will I then clear octree?
    //if(_octree != NULL)
	//	delete _octree;
    //_octree = NULL;
    _x_disp = 0;
    _x_var  = 0;
    _fim  = Eigen::Matrix6d::Zero();

    return true;
}

bool VisionBasedTunnelLocalizer::set_map(const pcl::PointCloud<pcl::PointXYZ>::Ptr &map){
    if(_octree != NULL)
		delete _octree;
    _octree = new pcl::octree::OctreePointCloudSearch<pcl::PointXYZ>(0.05);
    _octree->setInputCloud(map);
    _octree->addPointsFromInputCloud();
    return true;
}
// This function gets the optical flow trailers generated by '_history_len'
// frames, does ray-casting and geometrically estimates the world-x displacement.
// The displacement is between the poses '_history_len - 1' frames earlier and the
// last fed pose. Thus, it is users responsibility to incorporate the 
// non-uniform time step in the case '_history_len > 2'.
bool VisionBasedTunnelLocalizer::estimate_displacement(double &x_disp){
    // Fetch the features and ids from the feature tracker.
	// ### Later remove this, just an automated sanity check for myself
    assert(_trackes.size() == _cam_params.size());

    int num_trackers = _trackers.size();
    for(int t = 0 ; t < num_trackers ; t++){
        _trackers[t].get_features(   _feats[_feat_buffer_idx][t], 
                                  _feat_ids[_feat_buffer_idx][t], 0);
        cv::undistortPoints(_feats[_feat_buffer_idx][t], _feats[_feat_buffer_idx][t], 
                            _cam_params[t].camera_matrix, _cam_params[t].dist_coeffs);
    }

    // Reorder the '_feats[][]' so that feats. with the same ids align
    // at the same index.
    int num_tracked_feats = 0;
    for(int t = 0 ; t < num_trackers ; t++){
        for(int i = 0 ; i < _num_feats ; i++){
            int tail_id = _feat_ids[_feat_buffer_idx][t][i];
            int tip_id  = _feat_ids[!_feat_buffer_idx][t][i];
            if(tail_id == -1) 
                continue;
            if(tail_id == tip_id){
				num_tracked_feats++;
				continue;
            }
            for(int j = 0 ; j < _num_feats ; j++){
                tip_id = _feat_ids[!_feat_buffer_idx][t][j];
                if(tail_id == tip_id){
                    if(i != j){
                        std::swap(_feat_ids[!_feat_buffer_idx][t][i],
                                  _feat_ids[!_feat_buffer_idx][t][j]);
                        std::swap(_feats[!_feat_buffer_idx][t][i],
                                  _feats[!_feat_buffer_idx][t][j]);
                    }
					num_tracked_feats++;
                    break;
                }
            }
        }
    }

    // ### might need to plot the ids for check!

    // Back-project points with '_feat_ids[i] != -1'
    _of_tail_vecs.clear();
    _of_tip_vecs.clear();
    _of_tail_vecs.reserve(num_tracked_feats);
    _of_tip_vecs.reserve(num_tracked_feats);

    Eigen::Vector3f tail_dir, tip_dir;
	Eigen::Matrix3f tail_trans, tip_trans;

    Eigen::Vector3f tail_origin = _poses[!_feat_buffer_idx].topRightCorner<3, 1>().cast<float>();
    Eigen::Vector3f  tip_origin = _poses[ _feat_buffer_idx].topRightCorner<3, 1>().cast<float>();

    pcl::octree::OctreePointCloudSearch<pcl::PointXYZ>::AlignedPointTVector tail_proj, tip_proj;
    
    for(int t = 0 ; t < (int)_cam_params.size() ; t++){
        // Get the vector pointing along the feature in the world frame :
        // (1) - 2D -> 3D in camera frame
        // (2) - camera -> robot frame
        // (3) - robot -> world frame
        tail_trans = (_poses[!_feat_buffer_idx].topLeftCorner<3, 3>() * 
                        _cam_params[t].relative_pose.topLeftCorner<3, 3>() *  
                        _inv_camera_matrices[t]).cast<float>();
        tip_trans  = (_poses[_feat_buffer_idx].topLeftCorner<3, 3>() * 
                        _cam_params[t].relative_pose.topLeftCorner<3, 3>() *  
                        _inv_camera_matrices[t]).cast<float>();
        for(int i = 0; i < _num_feats ; i++){
            if(_feat_ids[_feat_buffer_idx][t][i] != -1 &&
               _feat_ids[_feat_buffer_idx][t][i] == _feat_ids[!_feat_buffer_idx][t][i]){
                tail_dir(0) = _feats[!_feat_buffer_idx][t][i].x;
                tail_dir(1) = _feats[!_feat_buffer_idx][t][i].y;
                tail_dir(2) = 1;
                tail_dir = tail_trans * tail_dir;
                tip_dir(0) = _feats[_feat_buffer_idx][t][i].x;
                tip_dir(1) = _feats[_feat_buffer_idx][t][i].y;
                tip_dir(2) = 1;
                tip_dir = tip_trans * tip_dir;
                _octree->getIntersectedVoxelCenters(tail_origin, tail_dir, tail_proj, 1);
                if(tail_proj.size() == 0)
                    continue;
                _octree->getIntersectedVoxelCenters( tip_origin,  tip_dir,  tip_proj, 1);
                if(tip_proj.size() == 0) 
                    continue;
                _of_tail_vecs.push_back(tail_proj[0]);
                _of_tip_vecs.push_back(tip_proj[0]);
            }    
        }
    }


    // Do the vector algebra for each optical flow vector.
    vector<double> x_disps;
    x_disps.resize(_of_tail_vecs.size());
    for(int t = 0 ; t < (int)_cam_params.size() ; t++){
        for(int i = 0; i < (int)_of_tail_vecs.size() ; i++){
            tail_dir(0) = _of_tail_vecs[i].x - tail_origin(0);
            tail_dir(1) = _of_tail_vecs[i].y - tail_origin(1);
            tail_dir(2) = _of_tail_vecs[i].z - tail_origin(2);
            tip_dir(0)  = _of_tip_vecs[i].x - tip_origin(0);
            tip_dir(1)  = _of_tip_vecs[i].y - tip_origin(1);
            tip_dir(2)  = _of_tip_vecs[i].z - tip_origin(2);
            double tail_dir_norm_sq = tail_dir.squaredNorm();
            double  tip_dir_norm_sq =  tip_dir.squaredNorm();
            double tail_dir_norm    = sqrt(tail_dir_norm_sq);
            double  tip_dir_norm    = sqrt( tip_dir_norm_sq);
            double cos_th = tail_dir.dot(tip_dir) / tail_dir_norm / tip_dir_norm;
            x_disps[i] = sqrt(tail_dir_norm_sq + tip_dir_norm_sq - 2 * tail_dir_norm * tip_dir_norm);
        }
    }

    _x_var = 0;
    std::sort(x_disps.begin(), x_disps.end());
    x_disp = _x_disp = x_disps[x_disps.size() / 2.0];
    for(int i = 0 ; i < (int)x_disps.size() ; i++)
        _x_var += pow(x_disps[i] - _x_disp, 2.0);
    _x_var /= x_disps.size();

    _feat_buffer_idx = !_feat_buffer_idx;

    return true;
}

bool VisionBasedTunnelLocalizer::get_displacement(double &x_disp){
    x_disp = _x_disp;
    return true;
}

bool VisionBasedTunnelLocalizer::get_back_projected_flow_vectors(vector<pcl::PointXYZ> &tails, vector<pcl::PointXYZ> &tips){
    tails = _of_tail_vecs;
    tips  = _of_tip_vecs;
    return true;
}

// This functions estimates the covariance of the displacement 
// estimate. It should be emphasized that the uncertainty regarding
// the x-coordinate is for displacement, not the position. Also 
// the off-diagonal elements encode the uncertainty cross-corelation 
// between the estimated x-disp. and the given y-z and yaw estimates.
// Thus the 6-by-6 covariance matrix has 1+2*3 = 7 non-zero element
// (1 for x-disp, 3 for cross-corelation between x and y-z-yaw).
bool VisionBasedTunnelLocalizer::get_covariance(Eigen::Matrix6d &cov){
	// ### to be implemented
    return true;
}

bool VisionBasedTunnelLocalizer::plot_flow(vector<cv::Mat> &imgs, bool plot_flow, bool plot_feat){
    assert(imgs.size() == _cam_params.size());
    for(int t = 0 ; t < (int)_cam_params.size() ; t++)
        _trackers[t].plot_flow(imgs[t], plot_flow, plot_feat);
    return true;
}

