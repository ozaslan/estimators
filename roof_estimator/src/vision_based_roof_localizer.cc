
#include "vision_based_roof_localizer.hh"
#include <opencv2/core/eigen.hpp>
#include <numeric>

#define debug(X) {cout << "---- " << X << endl; }
//#define debug(X) {}

VisionBasedRoofLocalizer::VisionBasedRoofLocalizer(int num_feats, int history_len, 
                       								string detector_type, double threshold){
	debug(__func__)
	_feat_buffer_idx = 0;
    _num_feats = num_feats;
    _of_history_len = history_len;
    _detector_type = detector_type;
    _feat_threshold = threshold;
    //_octree = NULL;
    _x_disp = 0;
    _x_var  = 0;
    _fim  = Eigen::Matrix6d::Zero();
    _extractor = UniformFeatureExtractor(_detector_type, 3,_num_feats, Size2i(4, 3), _feat_threshold, true);
    _num_frames_pushed = 0;
}

bool VisionBasedRoofLocalizer::_reset(){
	debug("Enter" << __func__)
	_trackers.clear();
    _feat_buffer_idx = 0;
    _feats[0].clear();
    _feats[1].clear();
    _feat_ids[0].clear();
    _feat_ids[1].clear();
    _cam_params.clear();
    _inv_camera_matrices.clear();
    _num_frames_pushed = 0;
    // ### How will I then clear octree?
    //if(_octree != NULL)
	//	delete _octree;
    //_octree = NULL;
    _x_disp = 0;
    _x_var  = 0;
    _fim  = Eigen::Matrix6d::Zero();

	debug("Exit" << __func__)
    return true;
}

bool VisionBasedRoofLocalizer::register_camera_params(const vector<CameraCalibParams> &params){
	debug("Enter " << __func__)
	// Reset '_trackers', '_inv_camera_matrices', '_feats' for a clean restart
    _reset();

	Eigen::Matrix3d temp_cam_mat;
    for(int i = 0 ; i < (int)params.size() ; i++){
        _cam_params.push_back(params[i]); // ### This is supposed to carry a deep copy.
        UniformFeatureTracker tracker(_num_feats, _of_history_len);
        _trackers.push_back(tracker);
        // Convert from OpenCV matrix to Eigen matrix.
		cv2eigen(params[i].camera_matrix, temp_cam_mat);
		// Calculate and store inverse camera matrices.
        _inv_camera_matrices.push_back(temp_cam_mat.inverse());
	}

	// Allocate memory for each tracker
	_feats[0].resize(params.size());
	_feats[1].resize(params.size());
	_feat_ids[0].resize(params.size());
	_feat_ids[1].resize(params.size());

	debug("Exit " << __func__)
    return true;
}

bool VisionBasedRoofLocalizer::push_camera_data(const vector<cv::Mat> &frames, const Eigen::Matrix4d &pose){
    // This function requires number of calibration data and 
    // size of 'frames' to be the same.
	debug(__func__)
    ASSERT(_cam_params.size() == frames.size(), "# of frames and camera parameters has to be the same.");

    // ### Parallelize this part with boost or p_thread.
    for(int t = 0 ; t < (int)_trackers.size() ; t++){
        _trackers[t].track_features(frames[t], _extractor, _cam_params[t].image_mask);
        // If the the previous frame has no features, we are starting 
        // from scratch.
        if(_feats[!_feat_buffer_idx][t].size() == 0){
            _trackers[t].get_features(   _feats[!_feat_buffer_idx][t], 
                                      _feat_ids[!_feat_buffer_idx][t], 0);
            cv::undistortPoints(_feats[!_feat_buffer_idx][t], _feats[!_feat_buffer_idx][t], 
                                _cam_params[t].camera_matrix, _cam_params[t].dist_coeffs,
                                cv::noArray(), _cam_params[t].camera_matrix);
            _poses[!_feat_buffer_idx] = pose;
        }
    }

    _num_frames_pushed++;
    _poses[_feat_buffer_idx] = pose;

    if(_num_frames_pushed > _of_history_len)
        cout << "WARNING!!! # of frame pushed is larger than the history length" << endl;

    return true;
}


bool VisionBasedRoofLocalizer::set_map(const pcl::PointCloud<pcl::PointXYZ>::Ptr &map){
	debug(__func__)
	// ### Update this to set_octree
    //if(_octree != NULL)
	//	delete _octree;
    //_octree = new pcl::octree::OctreePointCloudSearch<pcl::PointXYZ>(0.05);
    //_octree->setInputCloud(map);
    //_octree->addPointsFromInputCloud();
    return true;
}

// This function gets the optical flow trailers generated by '_history_len'
// frames, does ray-casting and geometrically estimates the world-x displacement.
// The displacement is between the poses '_history_len - 1' frames earlier and the
// last fed pose. Thus, it is users responsibility to incorporate the 
// non-uniform time step in the case '_history_len > 2'.
bool VisionBasedRoofLocalizer::estimate_displacement(double &x_disp){
    // Fetch the features and ids from the feature tracker.
	// ### Later remove this, just an automated sanity check for myself
	debug(__func__)
    ASSERT(_trackers.size() == _cam_params.size(), "Number of trackers and camera calibration paramters has to be the same.");
	
	if(_num_frames_pushed < (int)_trackers.size()){
		cout << "estimate_displacement(...) before data from all cameras is provided." << endl;
		x_disp = _x_disp = 0;
		_x_var = 0;
		return false;
	}

	// After the estimation, last pushed frame is still 'pushed'.
	_num_frames_pushed = 1;

	debug("A0")
    // ------------------------------------------------------------------------ //
    // Fetch the feature points from UniformFeatureTracker and undistort.
    int num_trackers = _trackers.size();
    for(int t = 0 ; t < num_trackers ; t++){
        _trackers[t].get_features(   _feats[_feat_buffer_idx][t], 
                                  _feat_ids[_feat_buffer_idx][t], 0);
        vector<Point2f> before = _feats[_feat_buffer_idx][t];
        cv::undistortPoints(_feats[_feat_buffer_idx][t], _feats[_feat_buffer_idx][t], 
                            _cam_params[t].camera_matrix, _cam_params[t].dist_coeffs,
                            cv::noArray(), _cam_params[t].camera_matrix);
    }

	debug("A1")
	// ------------------------------------------------------------------------ //
    // Reorder the '_feats[][]' so that feats. with the same ids align
    // at the same index.
    int num_tracked_feats = 0;
    for(int t = 0 ; t < num_trackers ; t++){
		cout << "t = " << t << endl;
        for(int i = 0 ; i < _num_feats ; i++){
            int tail_id = _feat_ids[_feat_buffer_idx][t][i];
            int tip_id  = _feat_ids[!_feat_buffer_idx][t][i];
            if(tail_id == -1) 
                continue;
            if(tail_id == tip_id){
				num_tracked_feats++;
				continue;
            }
            for(int j = 0 ; j < _num_feats ; j++){
                tip_id = _feat_ids[!_feat_buffer_idx][t][j];
                if(tail_id == tip_id){
                    if(i != j){
                        std::swap(_feat_ids[!_feat_buffer_idx][t][i],
                                  _feat_ids[!_feat_buffer_idx][t][j]);
                        std::swap(_feats[!_feat_buffer_idx][t][i],
                                  _feats[!_feat_buffer_idx][t][j]);
                    }
					num_tracked_feats++;
                    break;
                }
            }
        }
    }

	debug("A2")
	// Return if we do not have any matches
	if(num_tracked_feats == 0){
		_x_disp = x_disp = 0;
		_x_var = std::numeric_limits<double>::infinity();
		return false;
	}

	debug("A3")
	// ------------------------------------------------------------------------ //
    // Back-project points with '_feat_ids[i] != -1'    
    static Eigen::Vector3f tail_dir, tip_dir;
	static Eigen::Matrix3f tail_trans, tip_trans;

    static Eigen::Vector3f tail_origin;
    static Eigen::Vector3f  tip_origin;
 
    static pcl::octree::OctreePointCloudSearch<pcl::PointXYZ>::AlignedPointTVector tail_proj, tip_proj;

	tail_origin = _poses[!_feat_buffer_idx].topRightCorner<3, 1>().cast<float>();
    tip_origin  = _poses[ _feat_buffer_idx].topRightCorner<3, 1>().cast<float>();

	_of_tail_vecs.clear();
    _of_tip_vecs.clear();
    _of_tail_vecs.reserve(num_tracked_feats);
    _of_tip_vecs.reserve(num_tracked_feats);

	debug("A4")

	// '_x_disp' estimation related variables.
	double y_disp = _poses[0](1, 3) - _poses[1](1, 3);
    double z_disp = _poses[0](2, 3) - _poses[1](2, 3);
    double yz_disp_sq = (y_disp * y_disp) + (z_disp * z_disp);
    vector<double> x_disps;
    x_disps.reserve(_num_feats);

    for(int t = 0 ; t < (int)_cam_params.size() ; t++){
        // Get the vector pointing along the feature in the world frame :
        // (1) - 2D -> 3D in camera frame
        // (2) - camera -> robot frame
        // (3) - robot -> world frame

		// Concatenate transformation to obtain a single transformation matrix.
        tail_trans = (_poses[!_feat_buffer_idx].topLeftCorner<3, 3>() * 
                        _cam_params[t].relative_pose.topLeftCorner<3, 3>() *  
                        _inv_camera_matrices[t]).cast<float>();
        tip_trans  = (_poses[_feat_buffer_idx].topLeftCorner<3, 3>() * 
                        _cam_params[t].relative_pose.topLeftCorner<3, 3>() *  
                        _inv_camera_matrices[t]).cast<float>();

        for(int i = 0; i < _num_feats ; i++){
			// Check if the feature is valid.
            if(_feat_ids[_feat_buffer_idx][t][i] != -1 &&
               _feat_ids[_feat_buffer_idx][t][i] == _feat_ids[!_feat_buffer_idx][t][i]){
				// Transform the tail vector
                tail_dir(0) = _feats[!_feat_buffer_idx][t][i].x;
                tail_dir(1) = _feats[!_feat_buffer_idx][t][i].y;
                tail_dir(2) = 1;
                tail_dir = tail_trans * tail_dir;
				// Transform the tip vector
                tip_dir(0) = _feats[_feat_buffer_idx][t][i].x;
                tip_dir(1) = _feats[_feat_buffer_idx][t][i].y;
                tip_dir(2) = 1;
                tip_dir = tip_trans * tip_dir;
				// Do ray-casting for tail
                //_octree->getIntersectedVoxelCenters(tail_origin, tail_dir, tail_proj, 0);
                if(tail_proj.size() == 0)
                    continue;
				// Do ray-casting for tip
                //_octree->getIntersectedVoxelCenters( tip_origin,  tip_dir,  tip_proj, 0);
                if(tip_proj.size() == 0) 
                    continue;

				// Get the norm of the vectors pointing from the origins
				// of ray-casting to the points of intersection.
				double tail_proj_norm_sq = (pow(tail_proj[0].x - tail_origin(0), 2) +
									 	    pow(tail_proj[0].y - tail_origin(1), 2) +
									 	    pow(tail_proj[0].z - tail_origin(2), 2));
				double  tip_proj_norm_sq = (pow( tip_proj[0].x -  tip_origin(0), 2) +
									        pow( tip_proj[0].y -  tip_origin(1), 2) +
									        pow( tip_proj[0].z -  tip_origin(2), 2));
				double tail_proj_norm    = sqrt(tail_proj_norm_sq);
				double  tip_proj_norm    = sqrt( tip_proj_norm_sq);
	
				// Extend the direction vectors to the point of intersection.
				tail_dir *= tail_proj_norm / tail_dir.norm();
				tip_dir  *=  tip_proj_norm /  tip_dir.norm();
				
				// Carry the algebra for '_x_disp' estimation.
				double cos_th    = tail_dir.dot(tip_dir) / tail_proj_norm / tip_proj_norm;
	            double x_disp_sq = tail_proj_norm_sq + tip_proj_norm_sq - 2 * tail_proj_norm * tip_proj_norm * cos_th - yz_disp_sq;
	            if(x_disp_sq < 0)
					continue;

				int direction_of_motion = tail_dir(0) - tip_dir(0) > 0 ? 1 : -1;

				x_disps.push_back(direction_of_motion * sqrt(x_disp_sq));

				//cout << "x_disps[" << x_disps.size()-1 << "] = " << x_disps.back() << endl;
	            //cout << tail_proj_norm << ", " << tip_proj_norm << " " << cos_th << " " << yz_disp_sq << endl;

				// --------------------------------------------------------- //
				// This point-on, only store tip and tail vectors for visualization.
				tail_proj[0].x = tail_dir(0) + tail_origin(0);
				tail_proj[0].y = tail_dir(1) + tail_origin(1);
				tail_proj[0].z = tail_dir(2) + tail_origin(2);
				tip_proj[0].x = tip_dir(0) + tip_origin(0);
				tip_proj[0].y = tip_dir(1) + tip_origin(1);
				tip_proj[0].z = tip_dir(2) + tip_origin(2);

                _of_tail_vecs.push_back(tail_proj[0]);
                _of_tip_vecs.push_back(tip_proj[0]);
            }    
        }
    }

	debug("A4")
	// Calculate the variance and mean of the estimated x-displacements.
	// ### I should incorporate the uncertainty if yz estimate as well.
    _x_var  = 0;
    _x_disp = 0;

    for(int i = 0 ; i < (int)x_disps.size() ; i++)
		_x_disp += x_disps[i];
    _x_disp /= x_disps.size();

	// std::sort(x_disps.begin(), x_disps.end());
    // x_disp = _x_disp = x_disps[x_disps.size() / 2.0];

    for(int i = 0 ; i < (int)x_disps.size() ; i++)
        _x_var += pow(x_disps[i] - _x_disp, 2.0);

    _x_var /= x_disps.size();

	// Switch the buffer.
    _feat_buffer_idx = !_feat_buffer_idx;

	// ### erase belows
	cout << "_x_disp = " << _x_disp << endl;
	cout << "_x_var  = " << _x_var  << endl;
	if(isfinite(_x_disp))
		x_disp = _x_disp;
	else
		x_disp = 0;
	// ### 
	//_x_disp = x_disp = 0;

    return true;
}

bool VisionBasedRoofLocalizer::get_displacement(double &x_disp){
    x_disp = _x_disp;
    return true;
}

bool VisionBasedRoofLocalizer::get_back_projected_flow_vectors(vector<pcl::PointXYZ> &tails, vector<pcl::PointXYZ> &tips){
    tails = _of_tail_vecs;
    tips  = _of_tip_vecs;
    return true;
}

bool VisionBasedRoofLocalizer::get_covariance(Eigen::Matrix6d &cov){
	// ### to be implemented
    return true;
}

bool VisionBasedRoofLocalizer::plot_flows(vector<cv::Mat> &imgs, bool plot_details, bool plot_flow, bool plot_feats){
	debug(__func__)
    ASSERT(imgs.size() == _cam_params.size(), "Image vector and the parameter set has to be of same size.");
    for(int t = 0 ; t < (int)_cam_params.size() ; t++)
		ASSERT(imgs[t].cols * imgs[t].rows != 0, "Image size cannot be 0.");
    for(int t = 0 ; t < (int)_cam_params.size() ; t++){
		if(_num_frames_pushed <= _of_history_len && plot_details == true)
	        _trackers[t].plot_flow(imgs[t], plot_flow, plot_feats);
	    else {
			for(int i = 0 ; i < (int)_feats[_feat_buffer_idx][t].size() ; i++){
				if(_feat_ids[_feat_buffer_idx][t][i] == -1)
					continue;
				if(plot_flow == true)
					line(imgs[t], _feats[!_feat_buffer_idx][t][i], _feats[_feat_buffer_idx][t][i], Scalar(0, 255, 0), 1);
				if(plot_feats == true)
					circle(imgs[t],_feats[_feat_buffer_idx][t][i], 3, Scalar(0, 255, 0), -1);
			}
	    }
    }
    return true;
}

